{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from project_imports import *\n",
    "import importlib\n",
    "# project_imports = importlib.reload(project_imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = os.path.join( init_metadata.file_path_output, '*foreign*')\n",
    "load_file = glob.glob(file_name)\n",
    "load_file\n",
    "df_test = pd.read_csv(load_file[0])\n",
    "df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If the item has its own free spirit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the amount\n",
    "def ner_pos_tag_a_col(col_detail):\n",
    "    sent = nltk.word_tokenize(col_detail)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "ner_foreign_details = df_test.apply(lambda x: ner_pos_tag_a_col(x[1]), axis=1)\n",
    "ner_foreign_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ner_foreign_details)\n",
    "ner_foreign_details.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entry_ner = ner_foreign_details[0]\n",
    "test_entry_str = df_test.iloc[1, 1]\n",
    "test_entry_str\n",
    "test_entry_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date of foreign transaction occurs\n",
    "foreign_date = test_entry_str[0:8]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If the item includes processing fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ner_foreign_details[0][0][0]\n",
    "# result = re.match(r'\\d/\\d/\\d', test_entry_str)\n",
    "result = re.search(r'(?P<foreign_date>\\d+/\\d+/\\d+)\\s+(?P<foreign_amount>\\d+.\\d+)\\s+(?P<foreign_currency>\\w+)\\s+(?P<foreign_process>[\\w\\s]+)\\$\\s+(?P<fee_amt>\\d+.\\d+)\\s+(?P<foreign_process_currency>\\w+)', test_entry_str)\n",
    "\n",
    "# print(result)\n",
    "# type(result)\n",
    "result.group('foreign_process_currency')\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(s_result)\n",
    "s_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "break_foreign_details = lambda x: x.str.extract(r'(?P<foreign_date>\\d+/\\d+/\\d+)\\s+(?P<foreign_amount>\\d+.\\d+)\\s+(?P<foreign_currency>\\w+)\\s+(?P<foreign_process>[\\w\\s]+)\\$\\s+(?P<foreign_fee>\\d+.\\d+)\\s+(?P<foreign_process_currency>\\w+)')\n",
    "\n",
    "\n",
    "# def break_foreign_details(detail):\n",
    "#  return detail.str.extract(r'(?P<foreign_date>\\d+/\\d+/\\d+)\\s+(?P<foreign_amount>\\d+.\\d+)\\s+(?P<foreign_currency>\\w+)\\s+(?P<foreign_process>[\\w\\s]+)\\$\\s+(?P<roeign_fee>\\d+.\\d+)\\s+(?P<foreign_process_currency>\\w+)')\n",
    "\n",
    "df_foreign_details = df_test.apply(lambda x: break_foreign_details(df_test.loc[:,'Foreign_Details']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['id ', 'Foreign_Details'], dtype='object')"
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "# df_test.loc[:,'foreign_details']\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP, Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import *\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "test_str = 'Pak n Save Moorhou'\n",
    "# 1. remove puncuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('European', 'JJ'),\n ('authorities', 'NNS'),\n ('fined', 'VBD'),\n ('Google', 'NNP'),\n ('a', 'DT'),\n ('record', 'NN'),\n ('$', '$'),\n ('5.1', 'CD'),\n ('billion', 'CD'),\n ('on', 'IN'),\n ('Wednesday', 'NNP'),\n ('for', 'IN'),\n ('abusing', 'VBG'),\n ('its', 'PRP$'),\n ('power', 'NN'),\n ('in', 'IN'),\n ('the', 'DT'),\n ('mobile', 'JJ'),\n ('phone', 'NN'),\n ('market', 'NN'),\n ('and', 'CC'),\n ('ordered', 'VBD'),\n ('the', 'DT'),\n ('company', 'NN'),\n ('to', 'TO'),\n ('alter', 'VB'),\n ('its', 'PRP$'),\n ('practices', 'NNS')]"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "\n",
    "# ex = test_str\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(ex)\n",
    "sent    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S\n  European/JJ\n  authorities/NNS\n  fined/VBD\n  Google/NNP\n  (NP a/DT record/NN)\n  $/$\n  5.1/CD\n  billion/CD\n  on/IN\n  Wednesday/NNP\n  for/IN\n  abusing/VBG\n  its/PRP$\n  (NP power/NN)\n  in/IN\n  (NP the/DT mobile/JJ phone/NN)\n  (NP market/NN)\n  and/CC\n  ordered/VBD\n  (NP the/DT company/NN)\n  to/TO\n  alter/VB\n  its/PRP$\n  practices/NNS)\n[('European', 'JJ', 'O'),\n ('authorities', 'NNS', 'O'),\n ('fined', 'VBD', 'O'),\n ('Google', 'NNP', 'O'),\n ('a', 'DT', 'B-NP'),\n ('record', 'NN', 'I-NP'),\n ('$', '$', 'O'),\n ('5.1', 'CD', 'O'),\n ('billion', 'CD', 'O'),\n ('on', 'IN', 'O'),\n ('Wednesday', 'NNP', 'O'),\n ('for', 'IN', 'O'),\n ('abusing', 'VBG', 'O'),\n ('its', 'PRP$', 'O'),\n ('power', 'NN', 'B-NP'),\n ('in', 'IN', 'O'),\n ('the', 'DT', 'B-NP'),\n ('mobile', 'JJ', 'I-NP'),\n ('phone', 'NN', 'I-NP'),\n ('market', 'NN', 'B-NP'),\n ('and', 'CC', 'O'),\n ('ordered', 'VBD', 'O'),\n ('the', 'DT', 'B-NP'),\n ('company', 'NN', 'I-NP'),\n ('to', 'TO', 'O'),\n ('alter', 'VB', 'O'),\n ('its', 'PRP$', 'O'),\n ('practices', 'NNS', 'O')]\n"
    }
   ],
   "source": [
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)\n",
    "\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'ne_chunk' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a7529313043e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mne_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mne_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ne_chunk' is not defined"
     ]
    }
   ],
   "source": [
    "ne_tree = ne_chunk(pos_tag(word_tokenize(ex)))\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'ne_chunk' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d127de730f65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mne_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mne_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ne_chunk' is not defined"
     ]
    }
   ],
   "source": [
    "ne_tree = ne_chunk(pos_tag(word_tokenize(test_str)))\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "          'Text of first document.',\n",
    "          'Text of the second document made longer.',\n",
    "          'Number three.',\n",
    "          'This is number four.',\n",
    "]\n",
    "# learn the vocabulary and store CountVectorizer sparse matrix in X\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "# columns of X correspond to the result of this method\n",
    "vectorizer.get_feature_names() == (\n",
    "    ['document', 'first', 'four', 'is', 'longer',\n",
    "     'made', 'number', 'of', 'second', 'text',\n",
    "     'the', 'this', 'three'])\n",
    "# retrieving the matrix in the numpy form\n",
    "X.toarray()\n",
    "# transforming a new document according to learn vocabulary\n",
    "vectorizer.transform(['A new document.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('Pak n', 'PERSON')]\n"
    }
   ],
   "source": [
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "pprint([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[]\n"
    }
   ],
   "source": [
    "doc = nlp(test_str+' '*3)\n",
    "pprint([(X.text, X.label_) for X in doc.ents])\n",
    "# print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "LookupError",
     "evalue": "Could not find stanford-ner.jar jar file at /usr/share/stanford-ner/stanford-ner.jar",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-3b2abb696bab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n\u001b[0m\u001b[0;32m      5\u001b[0m                                            \u001b[1;34m'/usr/share/stanford-ner/stanford-ner.jar'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \t\t\t\t\t   encoding='utf-8')\n",
      "\u001b[1;32mC:\\Projects\\NeptuneX\\.venv_neptunex\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\NeptuneX\\.venv_neptunex\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;34m\"StanfordPOSTagger or StanfordNERTagger?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             )\n\u001b[1;32m---> 71\u001b[1;33m         self._stanford_jar = find_jar(\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         )\n",
      "\u001b[1;32mC:\\Projects\\NeptuneX\\.venv_neptunex\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    843\u001b[0m     \u001b[0mis_regex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m ):\n\u001b[1;32m--> 845\u001b[1;33m     return next(\n\u001b[0m\u001b[0;32m    846\u001b[0m         find_jar_iter(\n\u001b[0;32m    847\u001b[0m             \u001b[0mname_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Projects\\NeptuneX\\.venv_neptunex\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    730\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m             raise LookupError(\n\u001b[0m\u001b[0;32m    733\u001b[0m                 \u001b[1;34m\"Could not find %s jar file at %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m             )\n",
      "\u001b[1;31mLookupError\u001b[0m: Could not find stanford-ner.jar jar file at /usr/share/stanford-ner/stanford-ner.jar"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   '/usr/share/stanford-ner/stanford-ner.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')\n",
    "\n",
    "text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "\n",
    "print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standford\n",
    "https://pythonprogramming.net/named-entity-recognition-stanford-ner-tagger/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   '/usr/share/stanford-ner/stanford-ner.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')\n",
    "\n",
    "text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "\n",
    "print(classified_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": ".venv_neptunex",
   "display_name": "Python (.venv_neptunex)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}